{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YCRtmRa81WpLBAJUMaW0zCJU9DCTGyL1",
      "authorship_tag": "ABX9TyNyRkTKgosBp1CW8ZxBXs8N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svarunid/recipeye-ingredient-classification/blob/main/recipeye_ingredient_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ›’ Grocery Items Classification\n",
        "Preparing a DL model to calssify itemsin a grocery store.\n",
        "\n",
        "## Data\n",
        "The dataset from https://github.com/marcusklasson/GroceryStoreDataset contains images of fruits, vegetables and other packaged itemscommonly found in the grocery stores with their labels. The data is separated into train and test sets.\n",
        "\n",
        "\n",
        "## Evaluation\n",
        "For each image in the test set, predict a probability for each of the different classes.\n",
        "\n",
        "The evaluation will be made based on Multi Class Log Loss between 'Predicted probs' and 'Observed labels'."
      ],
      "metadata": {
        "id": "aTyllJSoy5Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "E15QxVt_JAFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n",
        "1. Read 'classes.csv' file and get the labels and label id mappings.\n",
        "2. Read the 'train.csv' and 'test.csv' to get paths of train & test images with associated labels."
      ],
      "metadata": {
        "id": "90u43En00fIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Recipeye/GroceryStoreDataset/dataset/classes.csv\")\n",
        "df = df[[\"Coarse Class Name (str)\",\"Coarse Class ID (int)\"]]\n",
        "df = df.drop_duplicates()\n",
        "df = df.set_index(\"Coarse Class ID (int)\")\n",
        "labels_dict = df.to_dict()[\"Coarse Class Name (str)\"]"
      ],
      "metadata": {
        "id": "x86rC_xtJBky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict"
      ],
      "metadata": {
        "id": "LSfDGT7ay542"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def import_df(path):\n",
        "  # txt as csv -> select specific cols -> name them.\n",
        "  df = pd.read_csv(path,\n",
        "                   header = None,\n",
        "                   names = [\"img_path\", \"coarse_id\"],\n",
        "                   usecols=[0,2])\n",
        "  # Shuffle the dataset\n",
        "  df = df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "cyq2tuY0YbZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/Recipeye/GroceryStoreDataset/dataset/\""
      ],
      "metadata": {
        "id": "PSG2Ebu1zKEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = import_df(data_path + \"train.txt\")\n",
        "test_df = import_df(data_path + \"test.txt\")"
      ],
      "metadata": {
        "id": "wM2QHlBMp8W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(list(labels_dict.keys()))"
      ],
      "metadata": {
        "id": "FdSdCD93zQgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data\n",
        "The data is split into train and validation set for training and validation."
      ],
      "metadata": {
        "id": "iES5VKCN0m5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [(data_path + img_path) for img_path in list(train_df[\"img_path\"])]\n",
        "y = [labels == label for label in train_df[\"coarse_id\"].to_numpy()]"
      ],
      "metadata": {
        "id": "eAJWhkL4aVk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "Mnykltp02hMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "Define a funtion to preprocess images\n",
        "1. Read images from gdrive as 3 channel img tensors.\n",
        "2. Noramlize tensors.\n",
        "3. Resize the tensors to a specific size based on the model used.\n"
      ],
      "metadata": {
        "id": "JyrBgbPo033P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The size to which the image has to be resized\n",
        "IMG_SIZE = 224\n",
        "\n",
        "def img_to_tensor(img_path,img_size=IMG_SIZE):\n",
        "  \"\"\"\n",
        "  Fetches the image from the file path, turns it into tensors\n",
        "  and scale them to the size specified.\n",
        "  \"\"\"\n",
        "  # Fetch the image from the img_path path\n",
        "  image = tf.io.read_file(img_path)\n",
        "  # Covert the image to tensors of 3 channel\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  # Normalize the tensor to have values ranging from 0(0) to 1(255)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  # Resize the image to the given size\n",
        "  image = tf.image.resize(image, size=[img_size,img_size])\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "1aruzF4X5bMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batching"
      ],
      "metadata": {
        "id": "bt2zYynSAlY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The size to each batch\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def get_img_label(img_path, label):\n",
        "  \"\"\"\n",
        "  Takes an img_path as input, processes it and returns a tuple (image, label).\n",
        "  \"\"\"\n",
        "  image = img_to_tensor(img_path)\n",
        "  return image, label\n",
        "\n",
        "def create_batch(X, y=None, batch_size=BATCH_SIZE, test_data=False, valid_data=False):\n",
        "  \"\"\"\n",
        "  Create a data batch of size same as batch_size.\n",
        "\n",
        "  Shuffle data if it's training set and do not shuffle validation data. \n",
        "  Also accepts test data as input.\n",
        "  \"\"\"\n",
        "  if test_data:\n",
        "    print(\"Processing test data..\")\n",
        "    # Create a dataset from the data.\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X)))\n",
        "    # Separate them into batches.\n",
        "    data_batch = data.map(img_to_tensor).batch(batch_size)\n",
        "  \n",
        "  if valid_data:\n",
        "    print(\"Processing validation data..\")\n",
        "    # Do not shuffle the data\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "                                               tf.constant(y)))\n",
        "    data_batch = data.map(get_img_label).batch(batch_size)\n",
        "\n",
        "  if not test_data and not valid_data:\n",
        "    print(\"Processing training data..\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "                                               tf.constant(y)))\n",
        "    # Shuffle the Data\n",
        "    data = data.shuffle(buffer_size=len(X))\n",
        "    data_batch = data.map(get_img_label).batch(batch_size)\n",
        "\n",
        "  return data_batch"
      ],
      "metadata": {
        "id": "yIwbmeK_AqKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation data batches\n",
        "train_data = create_batch(X_train, y_train)\n",
        "val_data = create_batch(X_val, y_val, valid_data=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMYl4b1mBCJ3",
        "outputId": "093aa6a4-585e-43b6-bf9f-00303af1f176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing training data..\n",
            "Processing validation data..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Models"
      ],
      "metadata": {
        "id": "voPG6bCCCFK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Input Size\n",
        "INPUT_SHAPE = (None, IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# Seutp output size\n",
        "OUTPUT_SIZE = len(labels)\n",
        "\n",
        "# Choosen model from tfHub\n",
        "MODEL_URL = \"https://tfhub.dev/sayakpaul/vit_s16_classification/1\""
      ],
      "metadata": {
        "id": "zOp5dFgeCH0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape=INPUT_SHAPE,output_shape=OUTPUT_SIZE,url=MODEL_URL):\n",
        "  \"\"\"\n",
        "  Create, compile and build a model from tfHub.\n",
        "  \"\"\"\n",
        "  print(\"Building model\", url)\n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(url),\n",
        "    tf.keras.layers.Dense(units=output_shape,\n",
        "                          activation=\"softmax\")\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "      optimizer=tf.keras.optimizers.Adam(),\n",
        "      metrics=[\"Accuracy\"]\n",
        "  )\n",
        "\n",
        "  # Build model\n",
        "  model.build(\n",
        "      input_shape\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "5YbgtVvBEtzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLDYrkfJExYg",
        "outputId": "e986052b-7390-4d01-d68b-d5f3c0e18ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model https://tfhub.dev/sayakpaul/vit_s16_classification/1\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_6 (KerasLayer)  (None, 1000)              22050664  \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 43)                43043     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,093,707\n",
            "Trainable params: 43,043\n",
            "Non-trainable params: 22,050,664\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "jIM-viKnFDZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_Accuracy\",\n",
        "                                                  mode=\"max\",\n",
        "                                                  patience=1)"
      ],
      "metadata": {
        "id": "OuIFcyfjFGVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 50 #@param {type:\"slider\",min:10,max:100,step:5}"
      ],
      "metadata": {
        "id": "u5p81LpEFHz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "  \"\"\"\n",
        "  Initialize a model and fit the data to the model\n",
        "  \"\"\"\n",
        "  model = create_model()\n",
        "\n",
        "  model.fit(x=train_data,\n",
        "            validation_data=val_data,\n",
        "            epochs=NUM_EPOCHS,\n",
        "            validation_freq=1,\n",
        "            callbacks=[early_stopping]\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "cEsnJLboFLWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhoDkqSjFOgm",
        "outputId": "805002e0-7afd-4052-b0c4-86431811c5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model https://tfhub.dev/sayakpaul/vit_s16_classification/1\n",
            "Epoch 1/50\n",
            "66/66 [==============================] - 537s 8s/step - loss: 1.6962 - Accuracy: 0.6037 - val_loss: 0.4629 - val_Accuracy: 0.8598\n",
            "Epoch 2/50\n",
            "66/66 [==============================] - 498s 8s/step - loss: 0.2557 - Accuracy: 0.9318 - val_loss: 0.2303 - val_Accuracy: 0.9432\n",
            "Epoch 3/50\n",
            "66/66 [==============================] - 495s 8s/step - loss: 0.1261 - Accuracy: 0.9749 - val_loss: 0.1566 - val_Accuracy: 0.9697\n",
            "Epoch 4/50\n",
            "66/66 [==============================] - 494s 8s/step - loss: 0.0779 - Accuracy: 0.9905 - val_loss: 0.1184 - val_Accuracy: 0.9773\n",
            "Epoch 5/50\n",
            "66/66 [==============================] - 488s 7s/step - loss: 0.0551 - Accuracy: 0.9938 - val_loss: 0.1089 - val_Accuracy: 0.9773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "YNqEjpF1TFwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "def save_model(model, suffix=None):\n",
        "  \"\"\"\n",
        "  Saves a given model with along with suffix to a directory.\n",
        "  \"\"\"\n",
        "  model_dir = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Recipeye/models\",\n",
        "                           datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  path = model_dir + suffix + \".h5\"\n",
        "  print(f\"Saving model to: {path}\")\n",
        "  tf.saved_model.save(model, path)"
      ],
      "metadata": {
        "id": "RLSQm_sKQY-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save_model(model, \"ViT-Adam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAEsoN4GQ4WG",
        "outputId": "5bbc36a1-7835-4a7b-a0a2-aaabb936fae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: /content/drive/MyDrive/Colab Notebooks/Recipeye/models/20230125-093428ViT-Adam.h5\n"
          ]
        }
      ]
    }
  ]
}