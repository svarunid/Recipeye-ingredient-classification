{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YCRtmRa81WpLBAJUMaW0zCJU9DCTGyL1",
      "authorship_tag": "ABX9TyNOn4G7oZ9vuETYW9R3vgb/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svarunid/Dog-Breed-Identification-using-TfHub-Model/blob/main/recipeye_ingredient_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ›’ Grocery Items Classification\n",
        "Preparing a DL model to classify itemsin a grocery store.\n",
        "\n",
        "## Data\n",
        "The dataset from https://github.com/marcusklasson/GroceryStoreDataset contains images of fruits, vegetables and other packaged itemscommonly found in the grocery stores with their labels. The data is separated into train and test sets.\n",
        "\n",
        "\n",
        "## Evaluation\n",
        "For each image in the test set, predict a probability for each of the different classes.\n",
        "\n",
        "The evaluation will be made based on Multi Class Log Loss between 'Predicted probs' and 'Observed labels'."
      ],
      "metadata": {
        "id": "aTyllJSoy5Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "E15QxVt_JAFc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n",
        "1. Read 'classes.csv' file and get the labels and label id mappings.\n",
        "2. Read the 'train.csv' and 'test.csv' to get paths of train & test images with associated labels."
      ],
      "metadata": {
        "id": "90u43En00fIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Recipeye/GroceryStoreDataset/dataset/classes.csv\")\n",
        "df = df[[\"Coarse Class Name (str)\",\"Coarse Class ID (int)\"]]\n",
        "df = df.drop_duplicates()\n",
        "df = df.set_index(\"Coarse Class ID (int)\")\n",
        "labels_dict = df.to_dict()[\"Coarse Class Name (str)\"]"
      ],
      "metadata": {
        "id": "x86rC_xtJBky"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict"
      ],
      "metadata": {
        "id": "LSfDGT7ay542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2afc5b46-914f-4b51-cb53-3e873e4a534b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Apple',\n",
              " 1: 'Avocado',\n",
              " 2: 'Banana',\n",
              " 3: 'Kiwi',\n",
              " 4: 'Lemon',\n",
              " 5: 'Lime',\n",
              " 6: 'Mango',\n",
              " 7: 'Melon',\n",
              " 8: 'Nectarine',\n",
              " 9: 'Orange',\n",
              " 10: 'Papaya',\n",
              " 11: 'Passion-Fruit',\n",
              " 12: 'Peach',\n",
              " 13: 'Pear',\n",
              " 14: 'Pineapple',\n",
              " 15: 'Plum',\n",
              " 16: 'Pomegranate',\n",
              " 17: 'Red-Grapefruit',\n",
              " 18: 'Satsumas',\n",
              " 19: 'Juice',\n",
              " 20: 'Milk',\n",
              " 21: 'Oatghurt',\n",
              " 22: 'Oat-Milk',\n",
              " 23: 'Sour-Cream',\n",
              " 24: 'Sour-Milk',\n",
              " 25: 'Soyghurt',\n",
              " 26: 'Soy-Milk',\n",
              " 27: 'Yoghurt',\n",
              " 28: 'Asparagus',\n",
              " 29: 'Aubergine',\n",
              " 30: 'Cabbage',\n",
              " 31: 'Carrots',\n",
              " 32: 'Cucumber',\n",
              " 33: 'Garlic',\n",
              " 34: 'Ginger',\n",
              " 35: 'Leek',\n",
              " 36: 'Mushroom',\n",
              " 37: 'Onion',\n",
              " 38: 'Pepper',\n",
              " 39: 'Potato',\n",
              " 40: 'Red-Beet',\n",
              " 41: 'Tomato',\n",
              " 42: 'Zucchini'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def import_df(path):\n",
        "  # txt as csv -> select specific cols -> name them.\n",
        "  df = pd.read_csv(path,\n",
        "                   header = None,\n",
        "                   names = [\"img_path\", \"coarse_id\"],\n",
        "                   usecols=[0,2])\n",
        "  # Shuffle the dataset\n",
        "  df = df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "cyq2tuY0YbZq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/Recipeye/GroceryStoreDataset/dataset/\""
      ],
      "metadata": {
        "id": "PSG2Ebu1zKEm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = import_df(data_path + \"train.txt\")\n",
        "test_df = import_df(data_path + \"test.txt\")"
      ],
      "metadata": {
        "id": "wM2QHlBMp8W1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(list(labels_dict.keys()))"
      ],
      "metadata": {
        "id": "FdSdCD93zQgG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data\n",
        "The data is split into train and validation set for training and validation."
      ],
      "metadata": {
        "id": "iES5VKCN0m5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [(data_path + img_path) for img_path in list(train_df[\"img_path\"])]\n",
        "y = [labels == label for label in train_df[\"coarse_id\"].to_numpy()]"
      ],
      "metadata": {
        "id": "eAJWhkL4aVk3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_test = [(data_path + img_path) for img_path in list(test_df[\"img_path\"])]\n",
        "y_test = [labels == label for label in test_df[\"coarse_id\"].to_numpy()]"
      ],
      "metadata": {
        "id": "Mnykltp02hMG"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "Define a funtion to preprocess images\n",
        "1. Read images from gdrive as 3 channel img tensors.\n",
        "2. Noramlize tensors.\n",
        "3. Resize the tensors to a specific size based on the model used.\n"
      ],
      "metadata": {
        "id": "JyrBgbPo033P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The size to which the image has to be resized\n",
        "IMG_SIZE = 224\n",
        "\n",
        "def img_to_tensor(img_path,img_size=IMG_SIZE):\n",
        "  \"\"\"\n",
        "  Fetches the image from the file path, turns it into tensors\n",
        "  and scale them to the size specified.\n",
        "  \"\"\"\n",
        "  # Fetch the image from the img_path path\n",
        "  image = tf.io.read_file(img_path)\n",
        "  # Covert the image to tensors of 3 channel\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  # Normalize the tensor to have values ranging from 0(0) to 1(255)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  # Resize the image to the given size\n",
        "  image = tf.image.resize(image, size=[img_size,img_size])\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "1aruzF4X5bMo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batching"
      ],
      "metadata": {
        "id": "bt2zYynSAlY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The size to each batch\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def get_img_label(img_path, label):\n",
        "  \"\"\"\n",
        "  Takes an img_path as input, processes it and returns a tuple (image, label).\n",
        "  \"\"\"\n",
        "  image = img_to_tensor(img_path)\n",
        "  return image, label\n",
        "\n",
        "def create_batch(X, y=None, batch_size=BATCH_SIZE, test_data=False, valid_data=False):\n",
        "  \"\"\"\n",
        "  Create a data batch of size same as batch_size.\n",
        "\n",
        "  Shuffle data if it's training set and do not shuffle validation data. \n",
        "  Also accepts test data as input.\n",
        "  \"\"\"\n",
        "  if test_data:\n",
        "    print(\"Processing test data..\")\n",
        "    # Create a dataset from the data.\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),tf.constant(y)))\n",
        "    # Separate them into batches.\n",
        "    data_batch = data.map(get_img_label).batch(batch_size)\n",
        "  \n",
        "  if valid_data:\n",
        "    print(\"Processing validation data..\")\n",
        "    # Do not shuffle the data\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "                                               tf.constant(y)))\n",
        "    data_batch = data.map(get_img_label).batch(batch_size)\n",
        "\n",
        "  if not test_data and not valid_data:\n",
        "    print(\"Processing training data..\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "                                               tf.constant(y)))\n",
        "    # Shuffle the Data\n",
        "    data = data.shuffle(buffer_size=len(X))\n",
        "    data_batch = data.map(get_img_label).batch(batch_size)\n",
        "\n",
        "  return data_batch"
      ],
      "metadata": {
        "id": "yIwbmeK_AqKR"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation data batches\n",
        "train_data = create_batch(X_train, y_train)\n",
        "val_data = create_batch(X_val, y_val, valid_data=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMYl4b1mBCJ3",
        "outputId": "2f4f807e-2d78-4730-cd2b-afe44f38c350"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing training data..\n",
            "Processing validation data..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = create_batch(X_test, y_test, test_data=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4dL94Rk3n5d",
        "outputId": "c26a1f34-0808-4f79-c175-bb43fabd6b21"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test data..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Models"
      ],
      "metadata": {
        "id": "voPG6bCCCFK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Input Size\n",
        "INPUT_SHAPE = (None, IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# Seutp output size\n",
        "OUTPUT_SIZE = len(labels)\n",
        "\n",
        "# Choosen model from tfHub\n",
        "MODEL_URL = \"https://tfhub.dev/sayakpaul/vit_s16_classification/1\""
      ],
      "metadata": {
        "id": "zOp5dFgeCH0r"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape=INPUT_SHAPE,output_shape=OUTPUT_SIZE,url=MODEL_URL):\n",
        "  \"\"\"\n",
        "  Create, compile and build a model from tfHub.\n",
        "  \"\"\"\n",
        "  print(\"Building model\", url)\n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(url),\n",
        "    tf.keras.layers.Dense(units=output_shape,\n",
        "                          activation=\"softmax\")\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "      optimizer=tf.keras.optimizers.Adam(),\n",
        "      metrics=[\"Accuracy\"]\n",
        "  )\n",
        "\n",
        "  # Build model\n",
        "  model.build(\n",
        "      input_shape\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "5YbgtVvBEtzY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLDYrkfJExYg",
        "outputId": "00eabe7b-0ae8-448d-c291-a69ef12ff98a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model https://tfhub.dev/sayakpaul/vit_s16_classification/1\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 1000)              22050664  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 43)                43043     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,093,707\n",
            "Trainable params: 43,043\n",
            "Non-trainable params: 22,050,664\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "jIM-viKnFDZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_Accuracy\",\n",
        "                                                  mode=\"max\",\n",
        "                                                  patience=1)"
      ],
      "metadata": {
        "id": "OuIFcyfjFGVa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 50 #@param {type:\"slider\",min:10,max:100,step:5}"
      ],
      "metadata": {
        "id": "u5p81LpEFHz5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "  \"\"\"\n",
        "  Initialize a model and fit the data to the model\n",
        "  \"\"\"\n",
        "  model = create_model()\n",
        "\n",
        "  model.fit(x=train_data,\n",
        "            validation_data=val_data,\n",
        "            epochs=NUM_EPOCHS,\n",
        "            validation_freq=1,\n",
        "            callbacks=[early_stopping]\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "cEsnJLboFLWt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhoDkqSjFOgm",
        "outputId": "dc9fd659-0c78-4978-e4c4-7cd94465be63"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model https://tfhub.dev/sayakpaul/vit_s16_classification/1\n",
            "Epoch 1/50\n",
            "66/66 [==============================] - 547s 8s/step - loss: 1.5251 - Accuracy: 0.6255 - val_loss: 0.4133 - val_Accuracy: 0.8617\n",
            "Epoch 2/50\n",
            "66/66 [==============================] - 502s 8s/step - loss: 0.2178 - Accuracy: 0.9479 - val_loss: 0.2004 - val_Accuracy: 0.9470\n",
            "Epoch 3/50\n",
            "66/66 [==============================] - 511s 8s/step - loss: 0.1096 - Accuracy: 0.9848 - val_loss: 0.1372 - val_Accuracy: 0.9773\n",
            "Epoch 4/50\n",
            "66/66 [==============================] - 502s 8s/step - loss: 0.0695 - Accuracy: 0.9905 - val_loss: 0.1158 - val_Accuracy: 0.9773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "YNqEjpF1TFwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "def save_model(model, suffix=None):\n",
        "  \"\"\"\n",
        "  Saves a given model with along with suffix to a directory.\n",
        "  \"\"\"\n",
        "  model_dir = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/Recipeye/models\",\n",
        "                           datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  path = model_dir + suffix\n",
        "  print(f\"Saving model to: {path}\")\n",
        "  model.save(path)\n",
        "  return path"
      ],
      "metadata": {
        "id": "RLSQm_sKQY-7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = save_model(model, \"ViT-Adam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAEsoN4GQ4WG",
        "outputId": "c78e7fe9-d9fc-4c7c-93d0-d5f671f03a4e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: /content/drive/MyDrive/Colab Notebooks/Recipeye/models/20230131-145857ViT-Adam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load & test the model\n",
        "Loading the saved model from the gdrive and testing the model on the test set."
      ],
      "metadata": {
        "id": "T6oOcUDtsrbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model(path, custom_objects={\"KerasLayer\": hub.KerasLayer})"
      ],
      "metadata": {
        "id": "qP9pm6uZBgoy"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A_e4zBDSkdj",
        "outputId": "8cf40fa1-b94b-4e09-ee80-f4744e7f1877"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f0fd39df9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.evaluate(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZohMSSUHSmM8",
        "outputId": "4ba490b4-8621-4df6-b5cc-e590e57c0adb"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78/78 [==============================] - 496s 6s/step - loss: 0.4792 - Accuracy: 0.8483\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.47923752665519714, 0.8482897281646729]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ]
}